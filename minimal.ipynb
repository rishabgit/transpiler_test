{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-01 22:23:08.365131: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-01 22:23:08.885195: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "WARNING:root:To preserve the compiler and transpiler caches across multiple machines, ensure that the relative path of your projects from the .ivy folder is consistent across all machines. You can do this by adding .ivy to your home folder and placing all projects in the same place relative to the home folder on all machines.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha access\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import PIL.Image\n",
    "import ivy\n",
    "import numpy as np\n",
    "import jax\n",
    "\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "\n",
    "class SimpleConvNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple Convolutional Neural Network\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 10, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(26 * 26 * 10, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 10),\n",
    "        )\n",
    "        \n",
    "        self.apply(self.initialize_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "    def predict(self, inp):\n",
    "        \"\"\"predict digit for input\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            raw_output = self(inp)\n",
    "            _, pred = torch.max(raw_output, 1)\n",
    "            return pred\n",
    "        \n",
    "    def initialize_weights(self, module):\n",
    "        if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = PIL.Image.open(\"0.png\")\n",
    "tensor_img = torch.from_numpy(np_img).float()\n",
    "tensor_img = tensor_img.unsqueeze(0).unsqueeze(0)\n",
    "tensor_img = torch.nn.functional.interpolate(tensor_img, size=28, mode='bicubic', align_corners=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rishab/anaconda3/envs/ivy/lib/python3.10/site-packages/six.py:87: DeprecationWarning: The Tix Tk extension is unmaintained, and the tkinter.tix wrapper module is deprecated in favor of tkinter.ttk\n",
      "  __import__(name)\n",
      "WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "/home/rishab/Documents/codes/unify/ivy/ivy/func_wrapper.py:234: UserWarning: Creating many views will lead to overhead when performing inplace updates with this backend\n",
      "  warnings.warn(\n",
      "/home/rishab/Documents/codes/unify/ivy/ivy/func_wrapper.py:234: UserWarning: Creating many views will lead to overhead when performing inplace updates with this backend\n",
      "  warnings.warn(\n",
      "/home/rishab/anaconda3/envs/ivy/lib/python3.10/site-packages/jax/_src/deprecations.py:51: DeprecationWarning: jax.numpy.DeviceArray is deprecated. Use jax.Array.\n",
      "  warnings.warn(message, DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "jax_graph = ivy.compiler.compiler.transpile(model, to=\"haiku\", args=(tensor_img,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rishab/Documents/codes/unify/ivy/ivy/func_wrapper.py:234: UserWarning: Creating many views will lead to overhead when performing inplace updates with this backend\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "to_flax_module.<locals>.TranspiledFlaxModule.__init__() got an unexpected keyword argument 'lazy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m jax_graph \u001b[39m=\u001b[39m ivy\u001b[39m.\u001b[39;49mcompiler\u001b[39m.\u001b[39;49mcompiler\u001b[39m.\u001b[39;49mtranspile(model, to\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mflax\u001b[39;49m\u001b[39m\"\u001b[39;49m, args\u001b[39m=\u001b[39;49m(tensor_img,))\n",
      "File \u001b[0;32m~/Documents/codes/unify/ivy/ivy/compiler/compiler.py:148\u001b[0m, in \u001b[0;36mtranspile\u001b[0;34m(source, to, debug_mode, with_numpy, args, kwargs, params_v, v, *objs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtranspile\u001b[39m(\n\u001b[1;32m    113\u001b[0m     \u001b[39m*\u001b[39mobjs: Callable,\n\u001b[1;32m    114\u001b[0m     source: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    121\u001b[0m     v\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,  \u001b[39m# Make this cleaner\u001b[39;00m\n\u001b[1;32m    122\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[Graph, LazyGraph]:\n\u001b[1;32m    123\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m    Transpile Callable objects passed as arguments. If args and kwargs are specified,\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[39m    transpilation is performed eagerly, otherwise, transpilation will happen lazily.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39m    Either a transpiled Graph or a non-initialized LazyGraph.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m     \u001b[39mreturn\u001b[39;00m _transpile(\n\u001b[1;32m    149\u001b[0m         \u001b[39m*\u001b[39;49mobjs,\n\u001b[1;32m    150\u001b[0m         source\u001b[39m=\u001b[39;49msource,\n\u001b[1;32m    151\u001b[0m         to\u001b[39m=\u001b[39;49mto,\n\u001b[1;32m    152\u001b[0m         debug_mode\u001b[39m=\u001b[39;49mdebug_mode,\n\u001b[1;32m    153\u001b[0m         with_numpy\u001b[39m=\u001b[39;49mwith_numpy,\n\u001b[1;32m    154\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    155\u001b[0m         kwargs\u001b[39m=\u001b[39;49mkwargs,\n\u001b[1;32m    156\u001b[0m         params_v\u001b[39m=\u001b[39;49mparams_v,\n\u001b[1;32m    157\u001b[0m         v\u001b[39m=\u001b[39;49mv,\n\u001b[1;32m    158\u001b[0m     )\n",
      "File \u001b[0;32mXXI.pyx:215\u001b[0m, in \u001b[0;36mXXI.transpile\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mVXI.pyx:372\u001b[0m, in \u001b[0;36mVXI._transpile_trainable_module\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mVXI.pyx:214\u001b[0m, in \u001b[0;36mVXI.to_flax_module\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: to_flax_module.<locals>.TranspiledFlaxModule.__init__() got an unexpected keyword argument 'lazy'"
     ]
    }
   ],
   "source": [
    "jax_graph = ivy.compiler.compiler.transpile(model, to=\"flax\", args=(tensor_img,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ivy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
